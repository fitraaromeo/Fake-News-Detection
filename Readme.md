# ğŸ“° Fake News Detection using LSTM, BERT, and DistilBERT

## ğŸ“Œ Project Description
This project is developed as part of the **Final Practical Exam (Ujian Akhir Praktikum / UAP)** for the **Machine Learning** course.

The objective of this project is to build a **fake news detection system** using **text-based machine learning models**, compare the performance between **non-pretrained models** and **pretrained (transfer learning) models**, and deploy the trained models into a **web-based application using Streamlit**.

---

## ğŸ“‚ Dataset
- **Dataset Name:** Fake and Real News Dataset  
- **Source:** Kaggle  
  https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset  

### Dataset Description
The dataset consists of two classes:
- **Fake News (label = 1):** misleading or hoax news articles  
- **Real News (label = 0):** factual and verified news articles  

Each data sample contains:
- `title` : headline of the news article  
- `text`  : full news content  

The dataset is merged and shuffled before preprocessing.

---

## ğŸ§¹ Data Preprocessing
The following preprocessing steps are applied:
1. Combining `title` and `text` into a single text field
2. Converting text to lowercase
3. Removing URLs
4. Removing non-alphabet characters
5. Removing extra whitespace

The dataset is split into:
- **80% Training data**
- **10% Validation data**
- **10% Testing data**

---

## ğŸ§  Models Implemented
This project implements **three machine learning models** as required by the UAP guidelines:

### 1ï¸âƒ£ LSTM (Non-Pretrained Model)
- Built from scratch using Recurrent Neural Network (LSTM) architecture
- Serves as a baseline model for text classification

### 2ï¸âƒ£ BERT (Pretrained Model â€“ Transfer Learning)
- Utilizes a pretrained **BERT-base-uncased** model
- Fine-tuned on the fake news dataset for binary classification

### 3ï¸âƒ£ DistilBERT (Pretrained Model â€“ Transfer Learning)
- A lighter and faster variant of BERT
- Used to compare performance and efficiency against BERT

---

## ğŸ“Š Model Evaluation
All models are evaluated using the **test dataset** with the following metrics:
- Accuracy
- Precision
- Recall
- F1-score
- Confusion Matrix

### ğŸ“ˆ Evaluation Results (Test Set)

| Model        | Accuracy | Precision | Recall | F1-score | Support |
|--------------|----------|-----------|--------|----------|---------|
| LSTM         | 1.00     | 1.00      | 1.00   | 1.00     | 4490    |
| BERT         | 1.00     | 1.00      | 1.00   | 1.00     | 4490    |
| DistilBERT   | 1.00     | 1.00      | 1.00   | 1.00     | 4490    |

**Class Distribution (Test Set):**
- Real News: 2142 samples  
- Fake News: 2348 samples  

---

## ğŸŒ Web Application (Streamlit)
A simple **web-based application** is developed using **Streamlit** to demonstrate the trained models.

### Application Features:
- Text input for news content
- Model selection via sidebar:
  - LSTM
  - BERT
  - DistilBERT
- Prediction output:
  - Classification label (Fake / Real)
  - Confidence score
  - Probability visualization (bar chart)
- Dashboard page:
  - Model comparison table
  - Confusion matrix visualization
- â€œHow to Useâ€ page for user guidance

---

## ğŸ“ Project Structure
Fake-News-Detection/
â”‚
â”œâ”€â”€ app/
â”‚ â””â”€â”€ app.py
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ preprocessing.py
â”‚ â”œâ”€â”€ infer_lstm.py
â”‚ â””â”€â”€ infer_transformer.py
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_eda.ipynb
â”‚ â”œâ”€â”€ 02_lstm_training.ipynb
â”‚ â”œâ”€â”€ 03_bert_training.ipynb
â”‚ â””â”€â”€ 04_distilbert_training.ipynb
â”‚
â”œâ”€â”€ models/ # (local only, not pushed to GitHub)
â”‚
â”œâ”€â”€ assets/
â”‚ â”œâ”€â”€ metrics.json
â”‚ â”œâ”€â”€ cm_lstm.png
â”‚ â”œâ”€â”€ cm_bert.png
â”‚ â””â”€â”€ cm_distilbert.png
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


---

## â–¶ï¸ How to Run the Application
1. Install dependencies:
```bash
pip install -r requirements.txt

2. Run the Streamlit application:
streamlit run app/app.py
Note: Trained models must be placed in the models/ directory locally before running the app.

## ğŸ› ï¸ Tech Stack
- Python
- TensorFlow (LSTM)
- PyTorch (BERT & DistilBERT)
- HuggingFace Transformers
- Scikit-learn
- Pandas & NumPy
- Streamlit
- Matplotlib

## ğŸ“Œ Notes
- This project is intended for academic purposes only.
- All implementations follow the UAP Machine Learning module guidelines.
- Pretrained models are used solely for educational and experimental purposes.

## ğŸ‘¨â€ğŸ“ Author

Fitra Romeo Winky
Machine Learning â€“ Final Practical Exam (UAP)